{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyanmic-Dialo\n",
    "\n",
    "Making a dynamic slot filling dialogue system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating and identifying slot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as extension task add the slot type\n",
    "def create_slot(slot_name, slot_model):\n",
    "    slot_model[slot_name] = \"unfilled\"\n",
    "\n",
    "def fill_slot(slot_name, slot_model, slot_value):\n",
    "    slot_model[slot_name] = slot_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_slot_func = {\n",
    "    \"name\": \"create_slot\",\n",
    "    \"description\": \"function creates a slots based on a user query\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"slots_to_add\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"name of the slot\"\n",
    "                },\n",
    "                \"description\": \"list of slots to add to accomplish what the user wants\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"slot_name\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "fill_slot_func = {\n",
    "    \"name\": \"fill_slot\",\n",
    "    \"description\": \"function fills a slot name with slot value\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"slots_to_fill\":{\n",
    "                \"type\": \"array\",\n",
    "                \"items\":{\n",
    "                    \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                        \"slot_name\":{\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"name of the slot\"\n",
    "                        },\n",
    "                        \"slot_value\":{\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"value of the slot\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"description\": \"list of slots to fill with their values\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"slot_name\", \"slot_value\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_slot_prompt = '''\n",
    "    You are an assistant that can determine whether to create a slot.\n",
    "    You are given a conversation context and a slot model.\n",
    "    The slot model is a list of slot names that already exist. \n",
    "    A slot name corresponds to a piece of information that the user can provide.\n",
    "    If the user asks a question or requests for information or an action that is not in the slot model then call create_slot_func and pass in a list of slot names designating the information the system would need answered by the user to accomplish what they request.\n",
    "    If necessary you can add multiple slots representing the different pieces of information the user would need to provide.\n",
    "    Use the entire conversation context as context but only take an action based on the last user message.\n",
    "    Do not create a slot if the user is asking for information that is already in the slot model.\n",
    "    If it is not necessary to create a slot then do nothing and do not call create_slot_func.\n",
    "'''\n",
    "\n",
    "fill_slot_prompt = '''\n",
    "    You are an assistant that can determine whether to fill a slot.\n",
    "    You are given a conversation context and a slot model.\n",
    "    The slot model is a list of slot names that need to be filled. \n",
    "    A slot name corresponds to a piece of information that the user can provide.\n",
    "    If the user answers with information answers a topic in the slot model then call fill_slot_func with the slot name and value.\n",
    "    Use the entire conversation context as context but only take an action based on the last user message.\n",
    "    Do not fill a slot if the user is not answering a question or providing information that is in the slot model.\n",
    "    If it is not necessary to fill a slot then do nothing and do not call fill_slot_func.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "def check_create_slot(convo_context, slot_model, remaining_slots):\n",
    "    tools_in = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": create_slot_func,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    user_prompt = f'''\n",
    "        Here is the conversation context:\n",
    "        {convo_context}\n",
    "        Here are the slots that already exist:\n",
    "        {\", \".join(remaining_slots)}\n",
    "    '''\n",
    "    \n",
    "    msgs_in = [\n",
    "        {\"role\": \"system\", \"content\": create_slot_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "        \n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=msgs_in,\n",
    "        tools=tools_in,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    tool_choice = completion.choices[0].message.tool_calls[0] if completion.choices[0].message.tool_calls else None\n",
    "    if tool_choice:\n",
    "        arguments = json.loads(tool_choice.function.arguments)\n",
    "        for slot in arguments[\"slots_to_add\"]:\n",
    "            create_slot(slot, slot_model)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fill_slot(convo_context, slot_model, remaining_slots):\n",
    "    tools_in = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": fill_slot_func,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    user_prompt = f'''\n",
    "        Here is the conversation context:\n",
    "        {convo_context}\n",
    "        Here are the slots that need to be filled:\n",
    "        {\", \".join(remaining_slots)}\n",
    "    '''\n",
    "    \n",
    "    msgs_in = [\n",
    "        {\"role\": \"system\", \"content\": fill_slot_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "        \n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=msgs_in,\n",
    "        tools=tools_in,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    tool_choice = completion.choices[0].message.tool_calls[0] if completion.choices[0].message.tool_calls else None\n",
    "\n",
    "    if tool_choice:\n",
    "        arguments = json.loads(tool_choice.function.arguments)\n",
    "        for elem in arguments[\"slots_to_fill\"]:\n",
    "            fill_slot(elem[\"slot_name\"], slot_model, elem[\"slot_value\"])\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_response(convo_context, slot_model, remaining_slots):\n",
    "#     check_create_slot(convo_context, slot_model, remaining_slots)\n",
    "#     check_fill_slot(convo_context, slot_model, remaining_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slot_model_in = [\"departure_date\", \"budget\", \"return_date\", \"preferred_airline\", \"destination_city\", \"departure_city\"]\n",
    "# remaining_slots = {}\n",
    "# convo_context_in = '''\n",
    "#     System: Hello how may I assist you in booking a flight today?\n",
    "#     User: I would like to book a flight to New York.\n",
    "#     System: What is your departure city?\n",
    "#     User: I am departing from Los Angeles.\n",
    "#     System: What is your budget?\n",
    "#     User: My budget is $500.\n",
    "# '''\n",
    "\n",
    "# analyze_response(convo_context_in, slot_model_in, remaining_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slot_model_in = [\"preferred_airline\"]\n",
    "# remaining_slots = {}\n",
    "# convo_context_in = '''\n",
    "#     System: Hello how may I assist you in booking a flight today?\n",
    "#     User: I would like to book a flight to New York.\n",
    "#     System: What is your departure city?\n",
    "#     User: I am departing from Los Angeles.\n",
    "#     System: What is your budget?\n",
    "#     User: My budget is $500.\n",
    "#     System: When is your departure date?\n",
    "#     User: I am departing on the 10th of November.\n",
    "#     System: What is your return date?\n",
    "#     User: I am returning on the 15th.\n",
    "#     System: Is there anything else you would like to add?\n",
    "#     User: Are you able to book a hotel for me?\n",
    "# '''\n",
    "\n",
    "# analyze_response(convo_context_in, slot_model_in, remaining_slots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## response generation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(task, slot_name=\"\", slot_value=\"\", previous_response=\"\"):\n",
    "    if task == \"regenerate\":\n",
    "        system_prompt = f'''\n",
    "        '''\n",
    "        user_prompt = f'''\n",
    "        '''\n",
    "        \n",
    "        \n",
    "    if task == \"find_slot\":\n",
    "        system_prompt = f'''\n",
    "            You are a system that will generate the next response based on the conversation context and a slot model.\n",
    "            Generate a response that will prompt the user to provide the value for the slot name.\n",
    "        '''\n",
    "        user_prompt = f'''\n",
    "            Conversation context: \n",
    "            {previous_response}\n",
    "            \n",
    "            Slot name: {slot_name}\n",
    "        '''\n",
    "        \n",
    "    if task == \"confirm\":\n",
    "        system_prompt = f'''\n",
    "            \n",
    "        '''\n",
    "        user_prompt = f'''\n",
    "        '''\n",
    "    \n",
    "    msgs_in = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "        \n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=msgs_in,\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(conversation, slot_model):\n",
    "    system_prompt = f'''\n",
    "        You are a system that will generate a response summarizing the information that has been collected.\n",
    "        Generate a response that summarizes the information in the slot model.\n",
    "        The slot model is an object of slot names and their corresponding values.\n",
    "        You will also be given the conversation context that was used to collect the information.\n",
    "        This summary message will be sent to the user to confirm the information that has been collected.\n",
    "        Respond with only the summary message.\n",
    "    '''\n",
    "    user_prompt = f'''\n",
    "        Conversation context: \n",
    "        {conversation}\n",
    "        \n",
    "        Slot model: \n",
    "        {slot_model}\n",
    "        '''\n",
    "    \n",
    "    msgs_in = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "        \n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=msgs_in,\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dialogue(start_state):\n",
    "    slot_model = start_state\n",
    "    remaining_slots = [key for key, value in slot_model.items() if value == \"unfilled\"]\n",
    "    conversation = \"\"\n",
    "    while remaining_slots:\n",
    "        slot_name = remaining_slots[0]\n",
    "        response = generate_response(\"find_slot\", slot_name)\n",
    "        conversation += f\"System: {response}\\n\"\n",
    "        print(response)\n",
    "        user_response = input()\n",
    "        conversation += f\"User: {user_response}\\n\"\n",
    "        created_slot = check_create_slot(conversation, slot_model, slot_model.keys())\n",
    "        remaining_slots = [key for key, value in slot_model.items() if value == \"unfilled\"]\n",
    "        filled_slot = check_fill_slot(conversation, slot_model, remaining_slots)\n",
    "        # response = generate_response(\"confirm\", slot_name, user_response)\n",
    "        # print(response)\n",
    "        remaining_slots = [key for key, value in slot_model.items() if value == \"unfilled\"]\n",
    "        print(slot_model)\n",
    "    summary_response = generate_summary(conversation, slot_model)\n",
    "    print(summary_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dialogue_no_input():\n",
    "    print(\"Enter a task request:\")\n",
    "    task_request = input()\n",
    "    tools_in = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": create_slot_func,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    system_prompt = f'''\n",
    "        Given a request from the user, determine what slots to create.\n",
    "        A slot is a piece of information necessary to accomplish what the user wants.\n",
    "        If the user asks a question or requests for information or an action then call create_slot_func and pass in a list of slot names designating the information the system would need answered by the user to accomplish what they request.\n",
    "    '''\n",
    "    \n",
    "    user_prompt = f'''\n",
    "        Here is the request:\n",
    "        {task_request}\n",
    "    '''\n",
    "    \n",
    "    msgs_in = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "        \n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=msgs_in,\n",
    "        tools=tools_in,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    tool_choice = completion.choices[0].message.tool_calls[0] if completion.choices[0].message.tool_calls else None\n",
    "    \n",
    "    slot_model = {}\n",
    "    \n",
    "    if tool_choice:\n",
    "        arguments = json.loads(tool_choice.function.arguments)\n",
    "        for slot in arguments[\"slots_to_add\"]:\n",
    "            create_slot(slot, slot_model)\n",
    "    \n",
    "    print(slot_model)\n",
    "    run_dialogue(slot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Could you please let me know the date you plan to depart?\n",
      "{'departure_date': '11/20/24', 'budget': 'unfilled', 'return_date': 'unfilled', 'preferred_airline': 'unfilled', 'destination_city': 'unfilled', 'departure_city': 'unfilled'}\n",
      "\n",
      "Sure! Could you please let me know what your budget is?\n",
      "{'departure_date': '11/20/24', 'budget': '$500', 'return_date': 'unfilled', 'preferred_airline': 'unfilled', 'destination_city': 'unfilled', 'departure_city': 'unfilled', 'hotel_included': 'unfilled'}\n",
      "When are you planning to return?\n"
     ]
    }
   ],
   "source": [
    "start_slots = {\"departure_date\": \"unfilled\", \"budget\": \"unfilled\", \"return_date\": \"unfilled\", \"preferred_airline\": \"unfilled\", \"destination_city\": \"unfilled\", \"departure_city\": \"unfilled\"}\n",
    "run_dialogue(start_slots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
